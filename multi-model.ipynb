{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import keras\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import clip\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Dropout, Flatten, Dense, Bidirectional\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPooling1D, Input, BatchNormalization, concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import skimage\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from skimage.util import random_noise\n",
    "\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def preprocess(data):\n",
    "    newData = []\n",
    "    for title in data:\n",
    "        title = re.sub(r'[0-9]+', '', title)\n",
    "        new = \" \"\n",
    "        for word in title.split(' '):\n",
    "            \n",
    "            if word not in stopwords:\n",
    "                new += lemmatizer.lemmatize(word) + ' '\n",
    "        newData.append(new)\n",
    "        \n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaximumLen(data):\n",
    "    maxL = 0\n",
    "    for tweet in data:\n",
    "        l = 0\n",
    "        for word in tweet.split(' '):\n",
    "            l += 1\n",
    "        if (l>maxL):\n",
    "            maxL = l    \n",
    "    return maxL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(pixels):\n",
    "    for test_image in data['Image Path']:\n",
    "        pixels = pixels.astype('float32')\n",
    "        mean, std = pixels.mean(), pixels.std()\n",
    "        pixels = (pixels - mean) / std\n",
    "        pixels = clip(pixels, 0, 1.0)\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image_path,size):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(imagepath):\n",
    "    images = []\n",
    "    for i in range(len(imagepath)):\n",
    "        image_mod = []\n",
    "        # read image, img is numpy array\n",
    "        img = resize_image(imagepath[i],74)\n",
    "        # append initial image to X\n",
    "        img = normalize_image(img)\n",
    "        image_mod.append(img)\n",
    "        \n",
    "        # APPLY AUGMENTATIONS\n",
    "        \n",
    "        # rotate image in all directions\n",
    "        image_mod.append(np.rot90(img))\n",
    "        image_mod.append(np.rot90(np.rot90(img)))\n",
    "        image_mod.append(np.rot90(np.rot90(np.rot90(img))))\n",
    "        \n",
    "        # flip image horizontally and vertically\n",
    "        image_mod.append(np.fliplr(img)) #horizontal\n",
    "        image_mod.append(np.flipud(img)) #vertical\n",
    "        \n",
    "        # add random noise to image\n",
    "        image_mod.append(random_noise(img))\n",
    "        \n",
    "        # blur image\n",
    "        image_mod.append(cv2.GaussianBlur(img, (11,11), 0))\n",
    "        \n",
    "        # zoom image\n",
    "        lx, ly = img.shape[0], img.shape[1]\n",
    "        cropped = img[lx // 4: - lx // 4, ly // 4: - ly // 4]\n",
    "        image_mod.append(cv2.resize(cropped, (74, 74), interpolation=cv2.INTER_AREA))\n",
    "\n",
    "        one = np.hstack((image_mod[0],image_mod[1],image_mod[2]))\n",
    "        two = np.hstack((image_mod[3],image_mod[4],image_mod[5]))\n",
    "        three = np.hstack((image_mod[6],image_mod[7],image_mod[8]))\n",
    "        output = np.vstack((one, two, three))\n",
    "        images.append(output)\n",
    "        \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Image Path\", \"Title\", \"Category ID\", \"Category\"]\n",
    "data = pd.read_csv('train.csv', encoding = \"ISO-8859-1\", header=None,\n",
    "                   usecols=[1,3,6,5], names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = data['Image Path']\n",
    "x = data['Title'].str.lower()\n",
    "y = to_categorical(LabelEncoder().fit_transform(data['Category']), num_classes = 30 )\n",
    "x = preprocess(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex = t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wordIndex) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedX = t.texts_to_sequences(x)\n",
    "maxlen = getMaximumLen(x)\n",
    "X = pad_sequences(encodedX, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embedding_vector = {}\n",
    "f = open('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle\n",
    "\n",
    "embeddings_index = dict()\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    key = values[0]\n",
    "    c = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[key] = c \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.zeros((vocab_size, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in t.word_index.items():\n",
    "    vector = embeddings_index.get(word)\n",
    "    if vector is not None:\n",
    "         embeddings_matrix[i] = vector\n",
    "    else:\n",
    "        embeddings_matrix[i] = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval, pathtrain, pathval = train_test_split(X, y, paths, train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagetrain = []\n",
    "for i in pathtrain:\n",
    "    img = resize_image('images/'+i,222)\n",
    "    imagetrain.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagetrain = np.array(imagetrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageval = []\n",
    "for i in pathval:\n",
    "    img = resize_image('images/'+i,222)\n",
    "    imageval.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageval = np.array(imageval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# #config.log_device_placement = True \n",
    "# config.intra_op_parallelism_threads=16\n",
    "# config.inter_op_parallelism_threads=16\n",
    "# sess = tf.Session(config=config)\n",
    "\n",
    "# from keras import backend as K\n",
    "# K.set_session(sess)\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in base_model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "\n",
    "top3_acc.__name__ = 'top3_acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Branch\n",
    "\n",
    "cnn_input = Input(shape=(222,222,3),name='cnn_input')\n",
    "vgg = base_model()(cnn_input)\n",
    "vgg = layers.Flatten()(vgg)\n",
    "vgg = layers.BatchNormalization()(vgg)\n",
    "vgg = layers.Dense(1024,  kernel_regularizer=regularizers.l2(0.001), activation='relu')(vgg)\n",
    "vgg = layers.BatchNormalization()(vgg)\n",
    "vgg = layers.Dropout(0.4)(vgg)\n",
    "vgg = layers.Dense(512, activation='relu')(vgg)\n",
    "vgg = layers.BatchNormalization()(vgg)\n",
    "vgg = layers.Dropout(0.4)(vgg)\n",
    "vgg = layers.Dense(256,  kernel_regularizer=regularizers.l2(0.001), activation='relu')(vgg)\n",
    "vgg = layers.Dropout(0.5)(vgg)\n",
    "\n",
    "# LSTM Branch\n",
    "\n",
    "lstm_input = Input(shape=(maxlen,), name='lstm_input')\n",
    "l0 = Embedding(vocab_size, 100, weights=[embeddings_matrix], name='l0')(lstm)\n",
    "l1 = Bidirectional(LSTM(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.3, name='l1'))(l0)\n",
    "l2 = keras.layers.GlobalMaxPool1D()(name='l2')(l1)\n",
    "l3 = Dense(50, activation=\"relu\", name='l3')(l2)\n",
    "\n",
    "merge = concatenate([vgg, l3],name='merge',axis=-1)\n",
    "d1 = Dense(32, activation=\"relu\", name='d1')(merge)\n",
    "predictions = Dense(30, activation='softmax', name='predictions')(d1)\n",
    "\n",
    "model = Model(inputs=[cnn_input, lstm_input], outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy',top3_acc, f1_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([imagetrain, xtrain], ytrain, \n",
    "                validation_data=([imageval, xval], yval), \n",
    "                epochs=10, batch_size=128, \n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=10),\n",
    "                           ModelCheckpoint(filepath='bestmodel.h5', monitor='val_loss', save_best_only=True)]\n",
    "                   )\n",
    "\n",
    "val_acc = history.history['val_acc']\n",
    "val_f1_m = history.history['val_f1_m']\n",
    "    \n",
    "model.save(\"multimodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', header=None, usecols=[1,2,3], names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtest = test['Image Path']\n",
    "xtest = preprocess(test['Title'].str.lower())\n",
    "ytest = to_categorical(LabelEncoder().fit_transform(test['Category']), num_classes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagetest = []\n",
    "for i in pathtest:\n",
    "    img = resize_image('iamges/'+i,222)\n",
    "    img = normalize_image(img)\n",
    "    imageval.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagetest = np.array(imagetest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = t.texts_to_sequences(xtest)\n",
    "Xtest = pad_sequences(Xtest, maxlen=maxlen, padding='post')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
