{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import keras\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Dropout, Flatten, Dense, Bidirectional\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPooling1D, Input, BatchNormalization, concatenate\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import skimage\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def preprocess(data):\n",
    "    newData = []\n",
    "    for title in data:\n",
    "        title = re.sub(r'[0-9]+', '', title)\n",
    "        new = \" \"\n",
    "        for word in title.split(' '):\n",
    "            \n",
    "            if word not in stopwords:\n",
    "                new += lemmatizer.lemmatize(word) + ' '\n",
    "        newData.append(new)\n",
    "        \n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaximumLen(data):\n",
    "    maxL = 0\n",
    "    for tweet in data:\n",
    "        l = 0\n",
    "        for word in tweet.split(' '):\n",
    "            l += 1\n",
    "        if (l>maxL):\n",
    "            maxL = l    \n",
    "    return maxL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Image Path\", \"Title\", \"Category ID\", \"Category\"]\n",
    "data = pd.read_csv('train.csv', encoding = \"ISO-8859-1\", header=None,\n",
    "                   usecols=[1,3,6,5], names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data['Image Path']\n",
    "x = data['Title'].str.lower()\n",
    "y = to_categorical(LabelEncoder().fit_transform(data['Category']), num_classes=30)\n",
    "x = preprocess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex = t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(wordIndex) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedX = t.texts_to_sequences(x)\n",
    "maxlen = getMaximumLen(x)\n",
    "X = pad_sequences(encodedX, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embedding_vector = {}\n",
    "f = open('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle\n",
    "\n",
    "embeddings_index = dict()\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    key = values[0]\n",
    "    c = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[key] = c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.zeros((vocab_size, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in t.word_index.items():\n",
    "    vector = embeddings_index.get(word)\n",
    "    if vector is not None:\n",
    "         embeddings_matrix[i] = vector\n",
    "    else:\n",
    "        embeddings_matrix[i] = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval, imagetrain, imageval = train_test_split(X, y, image, train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "\n",
    "top3_acc.__name__ = 'top3_acc'\n",
    "\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(vocab_size, 100, weights=[embeddings_matrix])(inp)\n",
    "x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(30, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(keras.optimizers.Adam(),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', top3_acc, f1_m])\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy',f1_m])\n",
    "\n",
    "history = model.fit(xtrain, ytrain, \n",
    "                validation_data=(xval, yval), \n",
    "                epochs=50, batch_size=128, \n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=10),\n",
    "                           ModelCheckpoint(filepath='lstm.h5', monitor='val_loss', save_best_only=True)]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import functools\n",
    "\n",
    "top3_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=3)\n",
    "\n",
    "top3_acc.__name__ = 'top3_acc'\n",
    "\n",
    "\n",
    "def create_model(params):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Embedding(vocab_size, 100, weights=[embeddings_matrix], input_length=maxlen, \n",
    "                                     trainable=False))\n",
    "    \n",
    "    for i in range(params['LSTM_layers']-1):\n",
    "        model.add(Bidirectional(LSTM(units=params['LSTM_units'], \n",
    "                                     return_sequences=True, dropout=params['LSTM_dropout'], \n",
    "                                     recurrent_dropout=params['recc_dropout'])))\n",
    "\n",
    "        \n",
    "    model.add(Bidirectional(LSTM(units=params['LSTM_units'], return_sequences=True, dropout=params['LSTM_dropout'], \n",
    "                                     recurrent_dropout=params['recc_dropout'])))\n",
    "    \n",
    "    model.add(keras.layers.GlobalMaxPool1D())\n",
    "    \n",
    "    for i in range(params['dense_layers']):\n",
    "        model.add(keras.layers.Dense(units=params['dense_units'], activation='relu'))\n",
    "        model.add(keras.layers.Dropout(params['dropout']))\n",
    "        \n",
    "    model.add(keras.layers.Dense(units=30, activation='softmax')) \n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',top3_acc, f1_m])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def random_search(params, n, xtrain, ytrain, xval, yval, epochs):\n",
    "    models = {}\n",
    "    i = 0\n",
    "    while (i<n):\n",
    "        print(\"Iteration number \" , i+1)\n",
    "        passing_param = {}\n",
    "        passing_param.update({'LSTM_dropout' : random.choice(params['LSTM_dropout'])})\n",
    "        passing_param.update({'recc_dropout' : random.choice(params['recc_dropout'])})\n",
    "        passing_param.update({'dropout' : random.choice(params['dropout'])})\n",
    "        passing_param.update({'LSTM_units' : random.choice(params['LSTM_units'])})\n",
    "        passing_param.update({'dense_units' : random.choice(params['dense_units'])})\n",
    "        passing_param.update({'batch_size' : random.choice(params['batch_size'])})\n",
    "        passing_param.update({'LSTM_layers' : random.choice(params['LSTM_layers'])})\n",
    "        passing_param.update({'dense_layers' : random.choice(params['dense_layers'])})\n",
    "        if str(passing_param) not in models:\n",
    "            i+=1\n",
    "            model = create_model(passing_param)\n",
    "            history = model.fit(xtrain, ytrain, validation_data=(xval, yval), epochs=epochs, \n",
    "                            batch_size=passing_param['batch_size'],\n",
    "                                callbacks=[EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                                         restore_best_weights=True),\n",
    "                            ModelCheckpoint(filepath='bestmodel'+str(i)+'.h5', \n",
    "                                            monitor='val_loss', save_best_only=True)])\n",
    "            \n",
    "            val_acc = history.history['val_acc']\n",
    "            val_f1_m = history.history['val_f1_m']\n",
    "            val_top3_acc = history.history['val_top3_acc']\n",
    "            models.update({str(passing_param): [val_acc, val_f1_m, val_top3_acc] })\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \n",
    "    'LSTM_dropout': [0.3, 0.4, 0.5],\n",
    "    'dropout' : [0.3, 0.4, 0.5],\n",
    "    'LSTM_units' : [100, 150, 200, 250],\n",
    "    'dense_units' : [32, 50, 128],\n",
    "    'batch_size' : [32, 64, 128],\n",
    "    'recc_dropout': [0.2, 0.3, 0.4],\n",
    "    'LSTM_layers' : [1, 2, 3],\n",
    "    'dense_layers' : [1, 2, 3],\n",
    "    }\n",
    "\n",
    "models = random_search(params, 15, xtrain, ytrain, xval, yval, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestAccuracy = 0\n",
    "BestF1 = 0\n",
    "BestHyp = []\n",
    "\n",
    "for key, value in models.items():\n",
    "    \n",
    "    f1 = 0\n",
    "    acc = 0\n",
    "    for epoch in range(10): \n",
    "        f1 += value[1][epoch]\n",
    "        acc += value[0][epoch]\n",
    "    avgF1 = f1/10\n",
    "    avgAcc = acc/10\n",
    "    \n",
    "    if (avgF1>BestF1):\n",
    "        BestF1 = value[1][9]\n",
    "        BestAccuracy = value[0][9]\n",
    "        BestHyp = key\n",
    "        \n",
    "print(\"Hyperparameters of the best model =  \" , BestHyp)\n",
    "print('Validation Accuracy is ', BestAccuracy, ' and Validation F-Mesure is ', BestF1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
